# -*- coding: utf-8 -*-
"""video_classifier_working.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rmxZle1hNLvAn67B7q_mZrctS6AvxOr6

# Video Classifier Using CNN and RNN
#!dir
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Imports Libraries and Loading Dataset

For training our data for classification, the data (Videos) are divided into two folders ( Clear and KnockDown) and split them 80 % for training and 20% for testing
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/train')

label_types = os.listdir('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/train')
print (label_types)

"""# Preparing Training Data"""

# import random
# rooms = []
# clear_counter=0

# for item in dataset_path:
#  # Get all the file names

#  all_rooms = os.listdir('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/train' + '/' +item)

#   # Add them to the list
#  for room in all_rooms:
#     if item=='Clear':
#       clear_counter+=1
#       if clear_counter==219 :
#         break
#       else:
#         rooms.append((item, str('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/train' + '/' +item) + '/' + room))
#     else:
#       rooms.append((item, str('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/train' + '/' +item) + '/' + room))

# # Build a dataframe
# train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
# print(train_df.head())
# print(train_df.tail())

# tag_counts = train_df['tag'].value_counts()
# print(tag_counts)

"""Create a list of training Data which is including of each video_name and its label as tag, then build a dataframe as csv file named train.csv"""

rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/train' + '/' +item) + '/' + room))

# Build a dataframe
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())

df = train_df.loc[:,['video_name','tag']]
df
df.to_csv('/content/drive/MyDrive/AI R&D Project/Equestrian/train.csv')

"""# Preparing Test Data
Create a list of testing Data which is including of each video_name and its label as tag, then build a dataframe as csv file named test.csv
"""

dataset_path = os.listdir('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/test')
print(dataset_path)

room_types = os.listdir('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/test')
print("Types of activities found: ", len(dataset_path))

rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/test' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/test' + '/' +item) + '/' + room))

# Build a dataframe
test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(test_df.head())
print(test_df.tail())

df = test_df.loc[:,['video_name','tag']]
df
df.to_csv('/content/drive/MyDrive/AI R&D Project/Equestrian/test.csv')

"""Loading Tensorflow libraries from its git"""

!pip install git+https://github.com/tensorflow/docs

from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os

"""Tensorflow Configuration to use 5GB  of GPU memory. It's a useful for managing GPU resources effectively."""

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])
  except RuntimeError as e:
    print(e)

"""Read CSV files containing training and testing data into pandas DataFrames and provides basic information about the size of the datasets. Show a sample of dataframe"""

train_df = pd.read_csv("/content/drive/MyDrive/AI R&D Project/Equestrian/train.csv")
test_df = pd.read_csv("/content/drive/MyDrive/AI R&D Project/Equestrian/test.csv")

print(f"Total videos for training: {len(train_df)}")
print(f"Total videos for testing: {len(test_df)}")


train_df.sample(10)

"""# Feed the videos through a network:

preprocess video data, including center cropping, resizing, and channel reordering, making them suitable for consumption by the models, particularly those dealing with video analysis or action recognition.
"""

# The following two methods are taken from this tutorial:
# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub
IMG_SIZE = 224


def crop_center_square(frame):
    y, x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]


def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]
            frames.append(frame)

            if len(frames) == max_frames:
                break
    finally:
        cap.release()
    return np.array(frames)

"""   ### Feature Extraction (CNN)
   Feature extractor model based on the InceptionV3 architecture using Keras
"""

def build_feature_extractor():
    feature_extractor = keras.applications.InceptionV3(
        weights="imagenet",
        include_top=False,
        pooling="avg",
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
    )
    preprocess_input = keras.applications.inception_v3.preprocess_input

    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
    preprocessed = preprocess_input(inputs)

    outputs = feature_extractor(preprocessed)
    return keras.Model(inputs, outputs, name="feature_extractor")


feature_extractor = build_feature_extractor()

"""### Label Encoding
StringLookup layer encode the class labels as integers. Convert 'Clear' Label to '0' and 'KnockDown' to '1'
"""

label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df["tag"]))
print(label_processor.get_vocabulary())

labels = train_df["tag"].values
labels = label_processor(labels[..., None]).numpy()
labels

"""Finally, we can put all the pieces together to create our data processing utility."""

#print(train_data[0].shape)
#train_data[0]

"""**Hyperparameters**"""

#Define hyperparameters

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 30

MAX_SEQ_LENGTH = 30
NUM_FEATURES = 2048

def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()

    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values

    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "train")
test_data, test_labels = prepare_all_videos(test_df, "test")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters

##### Save Data
import numpy as np
import pickle

def save_data(data, labels, filename_prefix):
    np.save(f"/content/drive/MyDrive/AI R&D Project/Equestrian/{filename_prefix}_data.npy", data)
    np.save(f"/content/drive/MyDrive/AI R&D Project/Equestrian/{filename_prefix}_labels.npy", labels)

# Save train and test data
save_data(train_data[0], train_labels, "train")
save_data(test_data[0], test_labels, "test")

# Save train and test masks
np.save("/content/drive/MyDrive/AI R&D Project/Equestrian/train_masks.npy", train_data[1])
np.save("/content/drive/MyDrive/AI R&D Project/Equestrian/test_masks.npy", test_data[1])

print("Data saved successfully.")

"""Save Data"""

import numpy as np
import pickle

def save_data(data, labels, filename_prefix):
    np.save(f"/content/drive/MyDrive/AI R&D Project/Equestrian/{filename_prefix}_data.npy", data)
    np.save(f"/content/drive/MyDrive/AI R&D Project/Equestrian/{filename_prefix}_labels.npy", labels)

# Save train and test data
save_data(train_data[0], train_labels, "train")
save_data(test_data[0], test_labels, "test")

# Save train and test masks
np.save("/content/drive/MyDrive/AI R&D Project/Equestrian/train_masks.npy", train_data[1])
np.save("/content/drive/MyDrive/AI R&D Project/Equestrian/test_masks.npy", test_data[1])

print("Data saved successfully.")

"""Load Data"""

def load_data(filename_prefix):
    data = np.load(f"/content/drive/MyDrive/AI R&D Project/Equestrian/{filename_prefix}_data.npy")
    labels = np.load(f"/content/drive/MyDrive/AI R&D Project/Equestrian/{filename_prefix}_labels.npy")
    return data, labels

# Load train and test data
train_data, train_labels = load_data("train")
test_data, test_labels = load_data("test")

# Load train and test masks
train_masks = np.load("/content/drive/MyDrive/AI R&D Project/Equestrian/train_masks.npy")
test_masks = np.load("/content/drive/MyDrive/AI R&D Project/Equestrian/test_masks.npy")

print(f"Loaded train data shape: {train_data.shape}")
print(f"Loaded train labels shape: {train_labels.shape}")
print(f"Loaded test data shape: {test_data.shape}")
print(f"Loaded test labels shape: {test_labels.shape}")
print(f"Loaded train masks shape: {train_masks.shape}")
print(f"Loaded test masks shape: {test_masks.shape}")

"""# The sequence model (RNN)
Now, we can feed these extracted features to a sequence model consisting of recurrent layers like GRU.
"""

train_labels.shape

# Import the learning rate scheduler
from keras.callbacks import ReduceLROnPlateau

# Define the learning rate scheduler callback
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)

"""Training Model and get the accuracy"""

# Utility for our sequence model.
def get_sequence_model():
    class_vocab = label_processor.get_vocabulary()

    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))
    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype="bool")

    # Refer to the following tutorial to understand the significance of using `mask`:
    # https://keras.io/api/layers/recurrent_layers/gru/
    # x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)
    # x = keras.layers.GRU(8)(x)
    # x = keras.layers.Dropout(0.4)(x)
    # x = keras.layers.Dense(8, activation="relu")(x)
    ###
    # Add more GRU layers

    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)
    x = keras.layers.GRU(16, return_sequences=True)(x)
    x = keras.layers.BatchNormalization()(x)  # Add BatchNormalization layer
    x = keras.layers.GRU(8)(x)
    x = keras.layers.BatchNormalization()(x)  # Add BatchNormalization layer
    # Add L2 regularization to the Dense layers
    x = keras.layers.Dense(8, activation="relu", kernel_regularizer=keras.regularizers.l2(0.01))(x)
    # Experiment with different dropout rates
    x = keras.layers.Dropout(0.5)(x)

    ####

    output = keras.layers.Dense(len(class_vocab), activation="softmax")(x)

    rnn_model = keras.Model([frame_features_input, mask_input], output)

    rnn_model.compile(
        loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"]
    )
    return rnn_model

EPOCHS = 10
# Utility for running experiments.
def run_experiment():
    filepath = "./tmp/video_classifier"
    checkpoint = keras.callbacks.ModelCheckpoint(
        filepath, save_weights_only=True, save_best_only=True, verbose=1
    )

    seq_model = get_sequence_model()
    history = seq_model.fit(
        [train_data, train_masks],
        train_labels,
        validation_split=0.3,
        epochs=EPOCHS,
        # callbacks=[checkpoint],
        callbacks=[checkpoint, reduce_lr],
    )
    #--------------------------------------------
    # history = seq_model.fit(
    #     [train_data[0], train_data[1]],
    #     train_labels,
    #     validation_split=0.3,
    #     epochs=EPOCHS,
    #     callbacks=[checkpoint],
    # )

    # seq_model.load_weights(filepath)
    # _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)
    #-------------------------------------------------

    seq_model.load_weights(filepath)
    _, accuracy = seq_model.evaluate([test_data, test_masks], test_labels)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")

    return history, seq_model


history, sequence_model = run_experiment()

import keras
from keras.layers import Input, GRU, Dropout, Dense, Masking
from keras.models import Model
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras.optimizers import Adam

def get_sequence_model():
    class_vocab = label_processor.get_vocabulary()
    MAX_SEQ_LENGTH = 30  # Define your MAX_SEQ_LENGTH
    NUM_FEATURES = 2048   # Define your NUM_FEATURES

    frame_features_input = Input((MAX_SEQ_LENGTH, NUM_FEATURES))
    mask_input = Input((MAX_SEQ_LENGTH,), dtype="bool")

    x = Masking(mask_value=0.0)(frame_features_input)  # Masking layer to handle variable sequence lengths
    x = GRU(64, return_sequences=True)(x)
    x = Dropout(0.5)(x)
    x = GRU(32)(x)
    x = Dropout(0.5)(x)
    x = Dense(32, activation="relu")(x)
    output = Dense(len(class_vocab), activation="softmax")(x)

    rnn_model = Model([frame_features_input, mask_input], output)

    optimizer = Adam(learning_rate=0.0001)
    rnn_model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
    return rnn_model

EPOCHS = 10

def run_experiment():
    filepath = "./tmp/video_classifier"
    checkpoint = ModelCheckpoint(filepath, save_weights_only=True, save_best_only=True, verbose=1)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)

    seq_model = get_sequence_model()
    history = seq_model.fit(
        [train_data, train_masks],
        train_labels,
        validation_split=0.3,
        epochs=EPOCHS,
        callbacks=[checkpoint, reduce_lr],
        batch_size=64  # Increase batch size for faster convergence
    )

    seq_model.load_weights(filepath)
    _, accuracy = seq_model.evaluate([test_data, test_masks], test_labels)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")

    return history, seq_model

history, sequence_model = run_experiment()

"""Plot Loss and Accuracy Graphs"""

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""# Inference

Prediction: Predict whether a video is a 'Clear' jump or unclear jump (Knockdown)
"""

def prepare_single_video(frames):
    frames = frames[None, ...]
    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32")

    for i, batch in enumerate(frames):
        video_length = batch.shape[0]
        length = min(MAX_SEQ_LENGTH, video_length)
        for j in range(length):
            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])
        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

    return frame_features, frame_mask


def sequence_prediction(path):
    class_vocab = label_processor.get_vocabulary()

    frames = load_video(os.path.join("test", path))
    frame_features, frame_mask = prepare_single_video(frames)
    probabilities = sequence_model.predict([frame_features, frame_mask])[0]

    for i in np.argsort(probabilities)[::-1]:
        print(f"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%")
    return frames

test_video = np.random.choice(test_df["video_name"].values.tolist())
print(f"Test video path: {test_video}")

test_frames = sequence_prediction(test_video)

t=test_df[test_df['tag'] == 'KnockDown']
test_video = np.random.choice(t["video_name"].values.tolist())
print(f"Test video path: {test_video}")

test_frames = sequence_prediction(test_video)

test_video = np.random.choice(test_df["video_name"].values.tolist())
print(f"Test video path: {test_video}")

test_frames = sequence_prediction(test_video)

test_video = "/content/drive/MyDrive/AI R&D Project/Equestrian/dataset_temp3/test/KnockDown/Tim Gredley - Belinda  - Lanaken - 1.45m - 01.04.2022.mp4_False_13.mp4"
print(f"Test video path: {test_video}")
# "D:\UNI\British_equistrian\Ammar\Chunks\Custom\train\failure\Ben Maher - Explosion W - Aachen - 1.60m - 03.07.2022 Round 2_False_10.mp4"
# "D:\UNI\British_equistrian\Ammar\Chunks\Custom\train\failure\Scott Brash - Hello Jefferson - Aachen - 1.60m - 03.07.2022 Round 2_False_0.mp4"
# "D:\UNI\British_equistrian\Ammar\Chunks\Custom\train\failure\Ben Maher - Explosion W - Aachen - 1.60m - 03.07.2022 Round 2.mp4_False_10.mp4"
test_frames = sequence_prediction(test_video)

from IPython.display import HTML

# Replace 'video.mp4' with the name of your .mp4 file
video_path = 'video.mp4'

# Generate HTML code to embed the video
html_code = f'''
<video width="640" height="480" controls>
  <source src="{test_video}" type="video/mp4">
</video>
'''

# Display the HTML code
HTML(html_code)

from IPython.display import HTML

HTML("""
    <video alt="train" width="520" height="440" controls>
        <source src="/content/drive/MyDrive/AI R&D Project/Equestrian/dataset/train/failure/Scott Brash - Hello Jefferson - Aachen - 1.60m - 03.07.2022 Round 2_False_0.mp4" type="video/mp4" style="height:300px;width:300px">
    </video>
""")